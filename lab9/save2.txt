#%% md

# Анализ данных осень 2021
# Практический анализ данных и ML соревнования

#%% md

### Зачем участвтовать

#%% md

- Полезно для резюме
- Полезно для скорости работы
- Иногда прибыльно
- Полезно для навыков, если вы не kaggle expert

#%% md

### Чему научили за курс

#%% md

- Быть линтером питона
- Перекладывать одни колонки данных в другие
    
    выделять признаки в данных, но не ясно какие из них полезны. И для чего полезны
    
- Применять непонятные ML модели к сырым данным

    алгоритмы AutoML делают похожую работу, но подбирают нужные параметры быстрее программистов
    
- Выделять на графиках закономерности, которые понятны без графиков

    сложно найти что-то в данных без цели. Проще предполагать по данным что может быть правдой, а что нет

#%% md

Вопросы без ответа

- Что считать хорошим анализом данных
- Что делать с плохим количеством данных
- Как смотреть на данные

#%% md

## Пример с соревнования

#%% md

Возьмём задачу из VK Cup 2020 года. Нужно:

- разобраться в данных, похожих на логи рекламных компаний соцсети
- придумать алгоритм оценки охвата аудитории
- посмотреть насколько точно предсказывается размер аудитории

#%% md

**Будет много графиков**

#%% md

#### Данные

#%%

!tree data/raw

#%% md

`users.tsv` - данные об аудитории == пользователях соцсети:
- `user_id`, `sex`, `age`, `city_id`

`history.tsv` - данные о рекламных компаниях:
- `hour` – в какой час пользователь видел объявление
- `cpm` - цена показанного рекламного объявления в рекламном аукционе. Это значит, что на данном аукционе это была максимальная ставка. 
- `publisher` - площадка, на который пользователь видел рекламу
- `user_id`

`validate_answers.tsv` - данные об охвате аудитории рекламной компании:
- `at_least_one` - доля пользователей, которая увидит объявление **хотя бы 1 раз**
- `at_least_two` - доля пользователей, которая увидит объявление **хотя бы 2 раза**
- `at_least_three` - доля пользователей, которая увидит объявление **хотя бы 3 раза**

#%% md

### Разберёмся в данных

#%%

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

plt.rcParams['figure.figsize'] = [14, 3]

#%% md

#### О пользователях

#%%

users = pd.read_csv('data/raw/users.csv')
print(f'dataset shape: {users.shape}')
users.head(7)

#%%

# Пользователи с какими id есть в данных?

users.user_id.hist(bins=100, rwidth=0.9)

#%%

# Пол пользователей

users.sex.hist(figsize=(3, 2))

#%%

# Возраст

users.age.hist(bins=100)

#%%

# Возраст основной аудитории

users[(users.age > 0) & (users.age < 60)].age.hist(bins=100)

#%%

# Города пользователей

print(f'Количество городов: {len(users.city_id.unique())}')
# Сколько пользователей в каждом городе
vc = users.value_counts('city_id')
vc.hist(bins=50)

#%%

# Логарифмированное количество жителей городов
log2_vc = np.log2(vc)
log2_vc.hist(bins=50)

#%% md

##### Менее тривиальные графики

#%%

# Города с населением более 1к
vc[vc > 1000].hist(bins=50)

#%%

# Исследуем жителей больших городов
vc[vc > 1000]

#%%

cities_users = users[users.city_id.isin([0, 3, 7])]
cities_users.shape

#%%

# Их возраст по городам

city_ages = cities_users[(cities_users.age > 0) & (cities_users.age < 70)].groupby('city_id').age.apply(list)

plt.figure(figsize=(7, 7))
plt.boxplot(city_ages)
plt.grid(True)
plt.legend([f'город {city_id}' for city_id in city_ages.index])
plt.show()

#%% md

#### О рекламных компаниях

#%%

history = pd.read_csv('data/raw/history.csv')
print(history.shape)
mean_cpm = np.mean(history.cpm)
# history[history['cpm'] > mean_cpm].size
history

#%%

# Время объявлений в ходе всего периода сбора данных
history.hour.hist(bins=200, figsize=(18, 4))

#%%

# Подробнее
history[history.hour < 150].hour.hist(bins=400, figsize=(18, 4))

#%%

# Цены объявлений
history.cpm.hist(bins=50)

#%%

# Логарифмированные
cpms = history.cpm.apply(np.log2)
cpms.hist(bins=100)

#%%

# Подробнее 
cpms[(cpms > 5) & (cpms < 10)].hist(bins=100)

#%%

# Что за платформы для рекламы? Сколько их?

print(len(history.publisher.unique()))
vc = history.publisher.value_counts()
big_publishers = set(vc.head(4).index)
big_publishers
# vc.hist(bins=50)
# plt.xlabel('Количество объявлений')
# plt.ylabel('Количество таких платформ')

#%%

# Сколько уникальных пользователей на платформах?

history.groupby('publisher').user_id.apply(lambda users: len(set(users))).hist(bins=50)
plt.xlabel('Количество пользователей')
plt.ylabel('Количество платформ')

#%%

# Какой трафик на каждой платформе?

# Посчитаем для каждого часа: сколько постов было на каждой платформе?

n_publishers = len(history.publisher.unique())
posts_per_hour_by_publishers = []
TAKE_FIRST_N_HOURS = 24 * 4

for hour, hour_data in history[history.hour < TAKE_FIRST_N_HOURS].groupby('hour'):
    publishers_posts = hour_data.value_counts('publisher')
    
    counts = [0 for _ in range(n_publishers)]
    for pub_id, posts_n in list(publishers_posts.items()):
        counts[pub_id-1] = posts_n
        
    posts_per_hour_by_publishers.append([hour] + counts)

    
df = pd.DataFrame.from_records(posts_per_hour_by_publishers, columns=['hour'] + list(history.publisher.unique()))
df.head()

#%%

df.plot.bar(x='hour', stacked=True, figsize=(20, 10))
plt.xlabel('Час')
plt.ylabel('Количество постов')

#%%

# И сколько постов посмотрел каждый пользователь?
vc = history.value_counts('user_id')
vc.hist(bins=50)

#%%

# Логарифмируем
np.log2(vc).hist(bins=50)
plt.xlabel('log2(просмотрено постов пользователем)')
plt.ylabel('Количество таких пользователей')

#%% md

#### Об результате рекламных компаний

#%%

posts = pd.read_csv('data/raw/ads.csv', converters={'user_ids': lambda x: list(map(int, x.split(","))),
                                                    'publishers': lambda x: list(map(int, x.split(",")))})
posts.head(2)
#%%

# Сколько по итогам аукциона стоит объявление
posts.cpm.hist(bins=100)

#%%

# Сколько часов длится рекламная компания

duration = (posts.hour_end - posts.hour_start)
duration.hist(bins=50)

#%%

np.log2(duration).hist(bins=50)
plt.xlabel('log(длительность в часах)')
plt.ylabel('количество объявлений')

#%%

# На какую аудиторию крутилось объявление

posts.audience_size.hist(bins=100)

#%% md

#### Об охвате

#%%

answers = pd.read_csv('data/raw/target.csv')
answers.head(7)

#%%

# Какая часть аудитории увидело объявление хоть раз
answers.at_least_one.hist(bins=50)

#%% md

Если у вас нет непонимания и вопросов о данных, то вы посмотрели хороший анализ данных. Возможно выше неплохой анализ

#%% md

### Построим модель

#%% md

#### Выделим числовые признаки

#%%

import seaborn as sns

#%%

# Соберём простую информацию по каждому объявлению в датасет
solution = pd.read_csv('data/raw/sample_solution.csv')

ads_train = pd.merge(posts, answers, on='ad_id')

ads_test = pd.merge(posts, solution, on='ad_id')
ads_train.head(2)

#%%

# Добавим пару простых признаков

def idToSex (id) :
    return users.iloc[id]['sex']

def idToAge (id) :
    return users.iloc[id]['age']

def cnt_big_publishers (arr) :
    cnt = 0
    for i in arr :
        if(big_publishers.__contains__(i)) :
            cnt += 1
    return cnt

#train
ads_train = ads_train.assign(n_publishers=ads_train.publishers.apply(lambda s: len(s)),
                 duration=ads_train.hour_end - ads_train.hour_start)

ads_train = ads_train.assign(user_sex=ads_train.user_ids.apply(lambda x: list(map(idToSex, x))))
ads_train = ads_train.assign(sex1=ads_train.user_sex.apply(lambda x: len(list(filter(lambda s : s == 1, x)))))

ads_train = ads_train.assign(user_age=ads_train.user_ids.apply(lambda x: list(map(idToAge, x))))
ads_train = ads_train.assign(mean_age=ads_train.user_age.apply(lambda x: np.mean(x)))

ads_train = ads_train.assign(big_publishers=ads_train.publishers.apply(cnt_big_publishers))

ads_test = ads_test.assign(n_publishers=ads_test.publishers.apply(lambda s: len(s)),
                 duration=ads_test.hour_end - ads_test.hour_start)

ads_test = ads_test.assign(user_sex=ads_test.user_ids.apply(lambda x: list(map(idToSex, x))))
ads_test = ads_test.assign(sex1=ads_test.user_sex.apply(lambda x: len(list(filter(lambda s : s == 1, x)))))

ads_test = ads_test.assign(user_age=ads_test.user_ids.apply(lambda x: list(map(idToAge, x))))
ads_test = ads_test.assign(mean_age=ads_test.user_age.apply(lambda x: np.mean(x)))

ads_test = ads_test.assign(big_publishers=ads_test.publishers.apply(cnt_big_publishers))

useful_columns = [
 'cpm',
 'duration',
 'n_publishers',
 'audience_size',
 'sex1',
 'mean_age',
 'big_publishers',
 'at_least_one'
]

ads_train = ads_train[useful_columns]
ads_test = ads_test[useful_columns]
ads_train.head()

#%%

def heatmap(data: pd.DataFrame):
    plt.figure(figsize=(10, 9))
    sns.heatmap(data.corr(), square=True, linecolor='white', annot=True)
    plt.yticks(rotation=30)
    plt.xticks(rotation=30)
    plt.show()

#%%

plt.figure(figsize=(10, 9))
heatmap(ads_train)

#%%

# Добавим дата саенс
ads_featured_train = ads_train.assign(
                          dur_x_audience=ads_train.duration*ads_train.audience_size,
                          log_dur_x_audience=np.log(ads_train.duration*ads_train.audience_size),
                          cpm_x_duration=ads_train.cpm*ads_train.duration,
                          sex_x_duration=np.log(ads_train.sex1*ads_train.duration),
                          age_x_duration=np.sqrt(ads_train.mean_age*ads_train.duration)
                         )
ads_featured_train = ads_featured_train.assign(
                           newtry=ads_featured_train.sex_x_duration*ads_featured_train.age_x_duration,
                           newtry2=np.log(ads_featured_train.cpm_x_duration+ads_featured_train.age_x_duration)
                         )

ads_featured_test = ads_test.assign(
                          dur_x_audience=ads_test.duration*ads_test.audience_size,
                          log_dur_x_audience=np.log(ads_test.duration*ads_test.audience_size),
                          cpm_x_duration=ads_test.cpm*ads_test.duration,
                          sex_x_duration=np.log(ads_test.sex1*ads_test.duration),
                          age_x_duration=np.sqrt(ads_test.mean_age*ads_test.duration)
                         )
ads_featured_test = ads_featured_test.assign(
                           newtry=ads_featured_test.sex_x_duration*ads_featured_test.age_x_duration,
                           newtry2=np.log(ads_featured_test.cpm_x_duration+ads_featured_test.age_x_duration)
                         )

heatmap(ads_featured_train)

#%% md

#### Обучим линейную регрессию

#%%

from typing import Tuple
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

#%%

def make_xy(data: pd.DataFrame) -> Tuple[pd.DataFrame, pd.Series]:
    x = data.drop('at_least_one', axis=1)
    y = data['at_least_one']
    return x, y

#%%

def mape(pred: np.ndarray, trueth: np.ndarray) -> np.ndarray:
    # mean_absolute_percentage_error with clipping huge values
    err = np.array(abs(pred - trueth) / trueth)
    err[err > 20] = 20 # clip 2000% error
    return err

#%%

# train, test = train_test_split(ads,
#                                test_size=0.2,
#                                random_state=42)
# train, test = train_test_split(ads.assign(
#                                           dur_x_audience=ads.duration*ads.audience_size,
#                                           log_dur_x_audience=np.log(ads.duration*ads.audience_size),
#                                           cpm_x_duration=ads.cpm*ads.duration
#                                          ),
#                                test_size=0.2,
#                                random_state=42)

# train, test = train_test_split(ads_featured,
#                                test_size=0.2,
#                                random_state=42)

train = ads_featured_train
test = ads_featured_test

# Обучим модель
x, y = make_xy(train)
model = LinearRegression().fit(x, y)

# Оценим ошибку
errors = mape(model.predict(x), y)
plt.hist(errors, bins=100)
plt.show()
print('Ошибка на train:', (errors).mean())

# Оценим ошибку на тесте
x, y = make_xy(test)
predicted = model.predict(x)
submission = solution
submission['at_least_one'] = predicted
errors = mape(predicted, y)
plt.hist(errors, bins=100)
print('Ошибка на test:', (errors).mean())

#%%

submission.to_csv('data/raw/submission.csv', index=False)
# solution

#%% md

### Насколько вообще разделимы данные?

#%%

import umap

#%%

# Покажем похожие объявления точками, а охват цветом
embedding = umap.UMAP().fit_transform(ads_train[['cpm', 'duration', 'n_publishers']])

plt.figure(figsize=(6, 5), dpi=150)

color = np.log1p(ads_train.at_least_one * 100)
plt.scatter(embedding[:, 0], embedding[:, 1], s=13, c=color)

#%%

# Покажем похожие объявления точками, а охват цветом. Но на данных с доп фичами
embedding = umap.UMAP().fit_transform(ads_featured_train[['cpm', 'duration', 'n_publishers', 'dur_x_audience', 'log_dur_x_audience', 'cpm_x_duration', 'newtry2', 'newtry']])

plt.figure(figsize=(6, 5), dpi=150)

color = np.log1p(ads_featured_train.at_least_one * 100)
plt.scatter(embedding[:, 0], embedding[:, 1], s=13, c=color)

#%% md

## Конец первой части курса

#%%


